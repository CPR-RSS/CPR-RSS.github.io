<?xml version="1.0" encoding="utf8"?>
<rss version="2.0">
<channel>
    <title>iclr 2020</title>
    
    <item>
        <title>Large Batch Optimization for Deep Learning: Training BERT in 76 minutes</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>SELF: Learning to Filter Noisy Labels with Self-Ensembling</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Reinforcement Learning Based Graph-to-Sequence Model for Natural Question Generation</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Sharing Knowledge in Multi-Task Deep Reinforcement Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>On the Weaknesses of Reinforcement Learning for Neural Machine Translation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>StructPool: Structured Graph Pooling via Conditional Random Fields</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Learning deep graph matching with channel-independent embedding and Hungarian attention</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Graph inference learning for semi-supervised classification</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards</title>
        <link>https://openreview.net/pdf?id=S1xKd24twB</link>
        <description>Learning to imitate expert behavior from demonstrations can be challenging, especially in environments with high-dimensional, continuous observations and unknown dynamics. Supervised learning methods based on behavioral cloning (BC) suffer from distribution shift: because the agent greedily imitates demonstrated actions, it can drift away from demonstrated states due to error accumulation. Recent methods based on reinforcement learning (RL), such as inverse RL and generative adversarial imitation learning (GAIL), overcome this issue by training an RL agent to match the demonstrations over a long horizon. Since the true reward function for the task is unknown, these methods learn a reward function from the demonstrations, often using complex and brittle approximation techniques that involve adversarial training. We propose a simple alternative that still uses RL, but does not require learning a reward function. The key idea is to provide the agent with an incentive to match the demonstrations over a long horizon, by encouraging it to return to demonstrated states upon encountering new, out-of-distribution states. We accomplish this by giving the agent a constant reward of r=+1 for matching the demonstrated action in a demonstrated state, and a constant reward of r=0 for all other behavior. Our method, which we call soft Q imitation learning (SQIL), can be implemented with a handful of minor modifications to any standard Q-learning or off-policy actor-critic algorithm. Theoretically, we show that SQIL can be interpreted as a regularized variant of BC that uses a sparsity prior to encourage long-horizon imitation. Empirically, we show that SQIL outperforms BC and achieves competitive results compared to GAIL, on a variety of image-based and low-dimensional tasks in Box2D, Atari, and MuJoCo. This paper is a proof of concept that illustrates how a simple imitation method based on RL with constant rewards can be as effective as more complex methods that use learned rewards.</description>
    </item>
    
    <item>
        <title>Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Mutual Mean-Teaching: Pseudo Label Refinery for Unsupervised Domain Adaptation on Person Re-identification</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Automatically Discovering and Learning New Visual Categories with Ranking Statistics</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Maxmin Q-learning: Controlling the Estimation Bias of Q-learning</title>
        <link>https://openreview.net/pdf?id=Bkg0u3Etwr</link>
        <description>Q-learning suffers from overestimation bias, because it approximates the maximum action value using the maximum estimated action value. Algorithms have been proposed to reduce overestimation bias, but we lack an understanding of how bias interacts with performance, and the extent to which existing algorithms mitigate bias. In this paper, we 1) highlight that the effect of overestimation bias on learning efficiency is environment-dependent; 2) propose a generalization of Q-learning, called \emph{Maxmin Q-learning}, which provides a parameter to flexibly control bias; 3) show theoretically that there exists a parameter choice for Maxmin Q-learning that leads to unbiased estimation with a lower approximation variance than Q-learning; and 4) prove the convergence of our algorithm in the tabular case, as well as convergence of several previous Q-learning variants, using a novel Generalized Q-learning framework. We empirically verify that our algorithm better controls estimation bias in toy environments, and that it achieves superior performance on several benchmark problems.</description>
    </item>
    
    <item>
        <title>Federated Adversarial Domain Adaptation</title>
        <link>https://openreview.net/pdf?id=HJezF3VYPB</link>
        <description>Federated learning improves data privacy and efficiency in machine learning performed over networks of distributed devices, such as mobile phones, IoT and wearable devices, etc. Yet models trained with federated learning can still fail to generalize to new devices due to the problem of domain shift. Domain shift occurs when the labeled data collected by source nodes statistically differs from the target node&apos;s unlabeled data. In this work, we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node. Our approach extends adversarial adaptation techniques to the constraints of the federated setting. In addition, we devise a dynamic attention mechanism and leverage feature disentanglement to enhance knowledge transfer. Empirically, we perform extensive experiments on several image and text classification tasks and show promising results under unsupervised federated domain adaptation setting.</description>
    </item>
    
    <item>
        <title>Depth-Adaptive Transformer</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Evaluating The Search Phase of Neural Architecture Search</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Diverse Trajectory Forecasting with Determinantal Point Processes</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>ProxSGD: Training Structured Neural Networks under Regularization and Constraints</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>LAMOL: LAnguage MOdeling for Lifelong Language Learning</title>
        <link>https://openreview.net/pdf?id=Skgxcn4YDS</link>
        <description>Most research on lifelong learning applies to images or games, but not language. We present LAMOL, a simple yet effective method for lifelong language learning (LLL) based on language modeling. LAMOL replays pseudo-samples of previous tasks while requiring no extra memory or model capacity. Specifically, LAMOL is a language model that simultaneously learns to solve the tasks and generate training samples. When the model is trained for a new task, it generates pseudo-samples of previous tasks for training alongside data for the new task. The results show that LAMOL prevents catastrophic forgetting without any sign of intransigence and can perform five very different language tasks sequentially with only one model. Overall, LAMOL outperforms previous methods by a considerable margin and is only 2-3% worse than multitasking, which is usually considered the LLL upper bound. The source code is available at https://github.com/jojotenya/LAMOL.</description>
    </item>
    
    <item>
        <title>Learning Expensive Coordination: An Event-Based Deep RL Approach</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Curvature Graph Network</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Distance-Based Learning from Errors for Confidence Calibration</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Deep Learning of Determinantal Point Processes via Proper Spectral Sub-gradient</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>N-BEATS: Neural basis expansion analysis for interpretable time series forecasting</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Automated Relational Meta-learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>To Relieve Your Headache of Training an MRF, Take AdVIL</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Linear Symmetric Quantization of Neural Networks for Low-precision Integer Hardware</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Weakly Supervised Clustering by Exploiting Unique Class Count</title>
        <link>https://openreview.net/pdf?id=B1xIj3VYvr</link>
        <description>A weakly supervised learning based clustering framework is proposed in this paper. As the core of this framework, we introduce a novel multiple instance learning task based on a bag level label called unique class count (ucc), which is the number of unique classes among all instances inside the bag. In this task, no annotations on individual instances inside the bag are needed during training of the models. We mathematically prove that with a perfect ucc classifier, perfect clustering of individual instances inside the bags is possible even when no annotations on individual instances are given during training. We have constructed a neural network based ucc classifier and experimentally shown that the clustering performance of our framework with our weakly supervised ucc classifier is comparable to that of fully supervised learning models where labels for all instances are known. Furthermore, we have tested the applicability of our framework to a real world task of semantic segmentation of breast cancer metastases in histological lymph node sections and shown that the performance of our weakly supervised framework is comparable to the performance of a fully supervised Unet model.</description>
    </item>
    
    <item>
        <title>Scalable and Order-robust Continual Learning with Additive Parameter Decomposition</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Continual Learning with Adaptive Weights (CLAW)</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Transferable Perturbations of Deep Feature Distributions</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>A Learning-based Iterative Method for Solving Vehicle Routing Problems</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>AutoQ: Automated Kernel-Wise Neural Network Quantization</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Understanding Architectures Learnt by Cell-based Neural Architecture Search</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>SVQN: Sequential Variational Soft Q-Learning Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Ranking Policy Gradient</title>
        <link>https://openreview.net/pdf?id=rJld3hEYvS</link>
        <description>Sample inefficiency is a long-lasting problem in reinforcement learning (RL). The state-of-the-art estimates the optimal action values while it usually involves an extensive search over the state-action space and unstable optimization. Towards the sample-efficient RL, we propose ranking policy gradient (RPG), a policy gradient method that learns the optimal rank of a set of discrete actions. To accelerate the learning of policy gradient methods, we establish the equivalence between maximizing the lower bound of return and imitating a near-optimal policy without accessing any oracles. These results lead to a general off-policy learning framework, which preserves the optimality, reduces variance, and improves the sample-efficiency. We conduct extensive experiments showing that when consolidating with the off-policy learning framework, RPG substantially reduces the sample complexity, comparing to the state-of-the-art.</description>
    </item>
    
    <item>
        <title>On Mutual Information Maximization for Representation Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Observational Overfitting in Reinforcement Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Enhancing Transformation-Based Defenses Against Adversarial Attacks with a Distribution Classifier</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Additive Powers-of-Two Quantization: An Efficient Non-uniform Discretization for Neural Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Lazy-CFR: fast and near-optimal regret minimization for extensive games with imperfect information</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Knowledge Consistency between Neural Networks and Beyond</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Image-guided Neural Object Rendering</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Implicit Bias of Gradient Descent based Adversarial Training on Separable Data</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>TabFact: A Large-scale Dataset for Table-based Fact Verification</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>ES-MAML: Simple Hessian-Free Meta Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Neural Stored-program Memory</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Multi-agent Reinforcement Learning for Networked System Control</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>FSPool: Learning Set Representations with Featurewise Sort Pooling</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Dynamically Pruned Message Passing Networks for Large-scale Knowledge Graph Reasoning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Theory and Evaluation Metrics for Learning Disentangled Representations</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Measuring Compositional Generalization: A Comprehensive Method on Realistic Data</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Rethinking Softmax Cross-Entropy Loss for Adversarial Robustness</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>The Implicit Bias of Depth: How Incremental Learning Drives Generalization</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget</title>
        <link>https://openreview.net/pdf?id=Hye1kTVFDS</link>
        <description>In many applications, it is desirable to extract only the relevant information from complex input data, which involves making a decision about which input features are relevant. The information bottleneck method formalizes this as an information-theoretic optimization problem by maintaining an optimal tradeoff between compression (throwing away irrelevant input information), and predicting the target. In many problem settings, including the reinforcement learning problems we consider in this work, we might prefer to compress only part of the input. This is typically the case when we have a standard conditioning input, such as a state observation, and a ``privileged&apos;&apos; input, which might correspond to the goal of a task, the output of a costly planning algorithm, or communication with another agent. In such cases, we might prefer to compress the privileged input, either to achieve better generalization (e.g., with respect to goals) or to minimize access to costly information (e.g., in the case of communication). Practical implementations of the information bottleneck based on variational inference require access to the privileged input in order to compute the bottleneck variable, so although they perform compression, this compression operation itself needs unrestricted, lossless access. In this work, we propose the variational bandwidth bottleneck, which decides for each example on the estimated value of the privileged information before seeing it, i.e., only based on the standard input, and then accordingly chooses stochastically, whether to access the privileged input or not. We formulate a tractable approximation to this framework and demonstrate in a series of reinforcement learning experiments that it can improve generalization and reduce access to computationally costly information.</description>
    </item>
    
    <item>
        <title>Learning the Arrow of Time for Problems in Reinforcement Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Reinforcement Learning with Competitive  Ensembles of Information-Constrained Primitives</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Robust Local Features for Improving the Generalization of Adversarial Training</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Analysis of Video Feature Learning in Two-Stream CNNs on the Example of Zebrafish Swim Bout Classification</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Learning Disentangled Representations for CounterFactual Regression</title>
        <link>https://openreview.net/pdf?id=HkxBJT4YvB</link>
        <description>We consider the challenge of estimating treatment effects from observational data; and point out that, in general, only some factors based on the observed covariates X contribute to selection of the treatment T, and only some to determining the outcomes Y. We model this by considering three underlying sources of {X, T, Y} and show that explicitly modeling these sources offers great insight to guide designing models that better handle selection bias. This paper is an attempt to conceptualize this line of thought and provide a path to explore it further. In this work, we propose an algorithm to (1) identify disentangled representations of the above-mentioned underlying factors from any given observational dataset D and (2) leverage this knowledge to reduce, as well as account for, the negative impact of selection bias on estimating the treatment effects from D. Our empirical results show that the proposed method achieves state-of-the-art performance in both individual and population based evaluation measures.</description>
    </item>
    
    <item>
        <title>Exploration in Reinforcement Learning with Deep Covering Options</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>AE-OT: A NEW GENERATIVE MODEL BASED ON EXTENDED SEMI-DISCRETE OPTIMAL TRANSPORT</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Logic and the 2-Simplicial Transformer</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Fooling Detection Alone is Not Enough: Adversarial Attack against Multiple Object Tracking</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>DivideMix: Learning with Noisy Labels as Semi-supervised Learning</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Improving Adversarial Robustness Requires Revisiting Misclassified Examples</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>V-MPO: On-Policy Maximum a Posteriori Policy Optimization for Discrete and Continuous Control</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Interpretable Complex-Valued Neural Networks for Privacy Protection</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Accelerating SGD with momentum for over-parameterized learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>A critical analysis of self-supervision, or what we can learn from a single image</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Disentangling Factors of Variations Using Few Labels</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Functional vs. parametric equivalence of ReLU networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Input Complexity and Out-of-distribution Detection with Likelihood-based Generative Models</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>RTFM: Generalising to New Environment Dynamics via Reading</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>What graph neural networks cannot learn: depth vs width</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Progressive Memory Banks for Incremental Domain Adaptation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Automated curriculum generation through setter-solver interactions</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>On Identifiability in Transformers</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Exploring Model-based Planning with Policy Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Geometric Insights into the Convergence of Nonlinear TD Learning</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Few-shot Text Classification with Distributional Signatures</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Escaping Saddle Points Faster with Stochastic Momentum</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Adversarial Policies: Attacking Deep Reinforcement Learning</title>
        <link>https://openreview.net/pdf?id=HJgEMpVFwB</link>
        <description>Deep reinforcement learning (RL) policies are known to be vulnerable to adversarial perturbations to their observations, similar to adversarial examples for classifiers. However, an attacker is not usually able to directly modify another agent&apos;s observations. This might lead one to wonder: is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment so as to create natural observations that are adversarial? We demonstrate the existence of adversarial policies in zero-sum games between simulated humanoid robots with proprioceptive observations, against state-of-the-art victims trained via self-play to be robust to opponents. The adversarial policies reliably win against the victims but generate seemingly random and uncoordinated behavior. We find that these policies are more successful in high-dimensional environments, and induce substantially different activations in the victim policy network than when the victim plays against a normal opponent. Videos are available at https://adversarialpolicies.github.io/.</description>
    </item>
    
    <item>
        <title>VideoFlow: A Conditional Flow-Based Model for Stochastic Video Generation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>GLAD: Learning Sparse Graph Recovery</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Pruned Graph Scattering Transforms</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Can gradient clipping mitigate label noise?</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Editable Neural Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>LEARNING EXECUTION THROUGH NEURAL CODE FUSION</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>FasterSeg: Searching for Faster Real-time Semantic Segmentation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Difference-Seeking Generative Adversarial Network--Unseen Sample Generation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Stochastic AUC Maximization with Deep Neural Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Semantically-Guided Representation Learning for Self-Supervised Monocular Depth</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>MACER: Attack-free and Scalable Robust Training via Maximizing Certified Radius</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Variational Recurrent Models for Solving Partially Observable Control Tasks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Population-Guided Parallel Policy Search for Reinforcement Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Compositional languages emerge in a neural iterated learning model</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Black-Box Adversarial Attack with Transferable Model-based Embedding</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>I Am Going MAD: Maximum Discrepancy Competition for Comparing Classifiers Adaptively</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Deep Network Classification by Scattering and Homotopy Dictionary Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Data-Independent Neural Pruning via Coresets</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Bounds on Over-Parameterization for Guaranteed Existence of Descent Paths in Shallow ReLU Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Novelty Detection Via Blurring</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Piecewise linear activations substantially shape the loss surfaces of neural networks</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Relational State-Space Model for Stochastic Multi-Object Systems</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Learning Efficient Parameter Server Synchronization Policies for Distributed SGD</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Action Semantics Network: Considering the Effects of Actions in Multiagent Systems</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Vid2Game: Controllable Characters Extracted from Real-World Videos</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Self-Adversarial Learning with Comparative Discrimination for Text Generation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Robust training with ensemble consensus</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Identifying through Flows for Recovering Latent Representations</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Certified Robustness for Top-k Predictions against Adversarial Perturbations via Randomized Smoothing</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Optimistic Exploration even with a Pessimistic Initialisation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>VL-BERT: Pre-training of Generic Visual-Linguistic Representations</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Ensemble Distribution Distillation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Gap-Aware Mitigation of Gradient Staleness</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Counterfactuals uncover the modular structure of deep generative models</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Physics-as-Inverse-Graphics: Unsupervised Physical Parameter Estimation from Video</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>An Inductive Bias for Distances: Neural Nets that Respect the Triangle Inequality</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>A Constructive Prediction of the Generalization Error Across Scales</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Scalable Neural Methods for Reasoning With a Symbolic Knowledge   Base</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>CLN2INV: Learning Loop Invariants with Continuous Logic Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>NAS evaluation is frustratingly hard</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Efficient and Information-Preserving Future Frame Prediction and Beyond</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Order Learning and Its Application to Age Estimation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Adversarially Robust Representations with Smooth Encoders</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>From Variational to Deterministic Autoencoders</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Computation Reallocation for Object Detection</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>A Fair Comparison of Graph Neural Networks for Graph Classification</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Generalization bounds for deep convolutional neural networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>SAdam: A Variant of Adam for Strongly Convex Functions</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Continual Learning with Bayesian Neural Networks for Non-Stationary Data</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Multiplicative Interactions and Where to Find Them</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>FEW-SHOT LEARNING ON GRAPHS VIA SUPER-CLASSES BASED ON GRAPH SPECTRAL MEASURES</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>On Computation and Generalization of Generative Adversarial Imitation Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>A Target-Agnostic Attack on Deep Models: Exploiting Security Vulnerabilities of Transfer Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Low-Resource Knowledge-Grounded Dialogue Generation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Deep 3D Pan via local adaptive "t-shaped" convolutions with global and local adaptive dilations</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Tree-Structured Attention with Hierarchical Accumulation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>The asymptotic spectrum of the Hessian of DNN throughout training</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Actor-Critic Provably Finds Nash Equilibria of Linear-Quadratic Mean-Field Games</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>In Search for a SAT-friendly Binarized Neural Network Architecture</title>
        <link>https://openreview.net/pdf?id=SJx-j64FDr</link>
        <description>Analyzing the behavior of neural networks is one of the most pressing challenges in deep learning. Binarized Neural Networks are an important class of networks that allow equivalent representation in Boolean logic and can be analyzed formally with logic-based reasoning tools like SAT solvers. Such tools can be used to answer existential and probabilistic queries about the network, perform explanation generation, etc. However, the main bottleneck for all methods is their ability to reason about large BNNs efficiently. In this work, we analyze architectural design choices of BNNs and discuss how they affect the performance of logic-based reasoners. We propose changes to the BNN architecture and the training procedure to get a simpler network for SAT solvers without sacrificing accuracy on the primary task. Our experimental results demonstrate that our approach scales to larger deep neural networks compared to existing work for existential and probabilistic queries, leading to significant speed ups on all tested datasets.</description>
    </item>
    
    <item>
        <title>Generative Ratio Matching Networks</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Learning to Represent Programs with Property Signatures</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>V4D: 4D Convolutional Neural Networks for Video-level Representation Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Option Discovery using Deep Skill Chaining</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Quantifying the Cost of Reliable Photo Authentication via High-Performance Learned Lossy Representations</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>On the Variance of the Adaptive Learning Rate and Beyond</title>
        <link>https://openreview.net/pdf?id=rkgz2aEKDr</link>
        <description>The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate -- its variance is problematically large in the early stage, and presume warmup works as a variance reduction technique. We provide both empirical and theoretical evidence to verify our hypothesis. We further propose Rectified Adam (RAdam), a novel variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the efficacy and robustness of RAdam.</description>
    </item>
    
    <item>
        <title>Dynamical Distance Learning for Semi-Supervised and Unsupervised Skill Discovery</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>A Theoretical Analysis of the Number of Shots in Few-Shot Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Unsupervised Model Selection for Variational Disentangled Representation Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Feature Interaction Interpretability: A Case for Explaining Ad-Recommendation Systems via Neural Interaction Detection</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Understanding the Limitations of Variational Mutual Information Estimators</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Language GANs Falling Short</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Stochastic Conditional Generative Networks with Basis Decomposition</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>LEARNED STEP SIZE QUANTIZATION</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>On the "steerability" of generative adversarial networks</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Reinforced active learning for image segmentation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Sign Bits Are All You Need for Black-Box Attacks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Deep Semi-Supervised Anomaly Detection</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Minimizing FLOPs to Learn Efficient Sparse Representations</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Reanalysis of Variance Reduced Temporal Difference Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Imitation Learning via Off-Policy Distribution Matching</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Augmenting Genetic Algorithms with Deep Neural Networks for Exploring the Chemical Space</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Identity Crisis: Memorization and Generalization Under Extreme Overparameterization</title>
        <link>https://openreview.net/pdf?id=B1l6y0VFPr</link>
        <description>We study the interplay between memorization and generalization of overparameterized networks in the extreme case of a single training example and an identity-mapping task. We examine fully-connected and convolutional networks (FCN and CNN), both linear and nonlinear, initialized randomly and then trained to minimize the reconstruction error. The trained networks stereotypically take one of two forms: the constant function (memorization) and the identity function (generalization). We formally characterize generalization in single-layer FCNs and CNNs. We show empirically that different architectures exhibit strikingly different inductive biases. For example, CNNs of up to 10 layers are able to generalize from a single example, whereas FCNs cannot learn the identity function reliably from 60k examples. Deeper CNNs often fail, but nonetheless do astonishing work to memorize the training output: because CNN biases are location invariant, the model must progressively grow an output pattern from the image boundaries via the coordination of many layers. Our work helps to quantify and visualize the sensitivity of inductive biases to architectural choices such as depth, kernel width, and number of channels.</description>
    </item>
    
    <item>
        <title>ReMixMatch: Semi-Supervised Learning with Distribution Matching and Augmentation Anchoring</title>
        <link>https://openreview.net/pdf?id=HklkeR4KPB</link>
        <description>We improve the recently-proposed ``MixMatch semi-supervised learning algorithm by introducing two new techniques: distribution alignment and augmentation anchoring. - Distribution alignment encourages the marginal distribution of predictions on unlabeled data to be close to the marginal distribution of ground-truth labels. - Augmentation anchoring} feeds multiple strongly augmented versions of an input into the model and encourages each output to be close to the prediction for a weakly-augmented version of the same input. To produce strong augmentations, we propose a variant of AutoAugment which learns the augmentation policy while the model is being trained. Our new algorithm, dubbed ReMixMatch, is significantly more data-efficient than prior work, requiring between 5 times and 16 times less data to reach the same accuracy. For example, on CIFAR-10 with 250 labeled examples we reach 93.73% accuracy (compared to MixMatch&apos;s accuracy of 93.58% with 4000 examples) and a median accuracy of 84.92% with just four labels per class.</description>
    </item>
    
    <item>
        <title>Adaptive Structural Fingerprints for Graph Attention Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>CAQL: Continuous Action Q-Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Learning Heuristics for Quantified Boolean Formulas through Reinforcement Learning</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Pure and Spurious Critical Points: a Geometric Study of Linear Networks</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Neural Text Generation With Unlikelihood Training</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Semi-Supervised Generative Modeling for Controllable Speech Synthesis</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Dynamic Time Lag Regression: Predicting What & When</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Scalable Model Compression by Entropy Penalized Reparameterization</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>AMRL: Aggregated Memory For Reinforcement Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Efficient Riemannian Optimization on the Stiefel Manifold via the Cayley Transform</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Unpaired Point Cloud Completion on Real Scans using Adversarial Training</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Adjustable Real-time Style Transfer</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Stochastic Weight Averaging in Parallel: Large-Batch Training That Generalizes Well</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Short and Sparse Deconvolution --- A Geometric Approach</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Selection via Proxy: Efficient Data Selection for Deep Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Global Relational Models of Source Code</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Detecting Extrapolation with Local Ensembles</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Learning to Link</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Adversarially robust transfer learning</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Overlearning Reveals Sensitive Attributes</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Bridging Mode Connectivity in Loss Landscapes and Adversarial Robustness</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Differentially Private Meta-Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Are Transformers universal approximators of sequence-to-sequence functions?</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Pre-training Tasks for Embedding-based Large-scale Retrieval</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Deep Imitative Models for Flexible Inference, Planning, and Control</title>
        <link>https://openreview.net/pdf?id=Skl4mRNYDr</link>
        <description>Imitation Learning (IL) is an appealing approach to learn desirable autonomous behavior. However, directing IL to achieve arbitrary goals is difficult. In contrast, planning-based algorithms use dynamics models and reward functions to achieve goals. Yet, reward functions that evoke desirable behavior are often difficult to specify. In this paper, we propose &quot;Imitative Models&quot; to combine the benefits of IL and goal-directed planning. Imitative Models are probabilistic predictive models of desirable behavior able to plan interpretable expert-like trajectories to achieve specified goals. We derive families of flexible goal objectives, including constrained goal regions, unconstrained goal sets, and energy-based goals. We show that our method can use these objectives to successfully direct behavior. Our method substantially outperforms six IL approaches and a planning-based approach in a dynamic simulated autonomous driving task, and is efficiently learned from expert demonstrations without online data collection. We also show our approach is robust to poorly-specified goals, such as goals on the wrong side of the road.</description>
    </item>
    
    <item>
        <title>CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Robust And Interpretable Blind Image Denoising Via Bias-Free Convolutional Neural Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>DeepV2D: Video to Depth with Differentiable Structure from Motion</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Learning Space Partitions for Nearest Neighbor Search</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Sign-OPT: A Query-Efficient Hard-label Adversarial Attack</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>RaCT: Toward Amortized Ranking-Critical Training For Collaborative Filtering</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Intrinsic Motivation for Encouraging Synergistic Behavior</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Chameleon: Adaptive Code Optimization for Expedited Deep Neural Network Compilation</title>
        <link>https://openreview.net/pdf?id=rygG4AVFvH</link>
        <description>Achieving faster execution with shorter compilation time can foster further diversity and innovation in neural networks. However, the current paradigm of executing neural networks either relies on hand-optimized libraries, traditional compilation heuristics, or very recently genetic algorithms and other stochastic methods. These methods suffer from frequent costly hardware measurements rendering them not only too time consuming but also suboptimal. As such, we devise a solution that can learn to quickly adapt to a previously unseen design space for code optimization, both accelerating the search and improving the output performance. This solution dubbed Chameleon leverages reinforcement learning whose solution takes fewer steps to converge, and develops an adaptive sampling algorithm that not only focuses on the costly samples (real hardware measurements) on representative points but also uses a domain-knowledge inspired logic to improve the samples itself. Experimentation with real hardware shows that Chameleon provides 4.45x speed up in optimization time over AutoTVM, while also improving inference time of the modern deep networks by 5.6%.</description>
    </item>
    
    <item>
        <title>Recurrent neural circuits for contour detection</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Locality and Compositionality in Zero-Shot Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Understanding Knowledge Distillation in Non-autoregressive Machine Translation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Thieves on Sesame Street! Model Extraction of BERT-based APIs</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Fast is better than free: Revisiting adversarial training</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>DBA: Distributed Backdoor Attacks against Federated Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>DeFINE: Deep Factorized Input Token Embeddings for Neural Sequence Modeling</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Sampling-Free Learning of Bayesian Quantized Neural Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Learning to solve the credit assignment problem</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Four Things Everyone Should Know to Improve Batch Normalization</title>
        <link>https://openreview.net/pdf?id=HJx8HANFDH</link>
        <description>A key component of most neural network architectures is the use of normalization layers, such as Batch Normalization. Despite its common use and large utility in optimizing deep architectures, it has been challenging both to generically improve upon Batch Normalization and to understand the circumstances that lend themselves to other enhancements. In this paper, we identify four improvements to the generic form of Batch Normalization and the circumstances under which they work, yielding performance gains across all batch sizes while requiring no additional computation during training. These contributions include proposing a method for reasoning about the current example in inference normalization statistics, fixing a training vs. inference discrepancy; recognizing and validating the powerful regularization effect of Ghost Batch Normalization for small and medium batch sizes; examining the effect of weight decay regularization on the scaling and shifting parameters and ; and identifying a new normalization algorithm for very small batch sizes by combining the strengths of Batch and Group Normalization. We validate our results empirically on six datasets: CIFAR-100, SVHN, Caltech-256, Oxford Flowers-102, CUB-2011, and ImageNet.</description>
    </item>
    
    <item>
        <title>Pseudo-LiDAR++: Accurate Depth for 3D Object Detection in Autonomous Driving</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>SlowMo: Improving Communication-Efficient Distributed SGD with Slow Momentum</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>MetaPix: Few-Shot Video Retargeting</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Learning to Learn by Zeroth-Order Oracle</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames</title>
        <link>https://openreview.net/pdf?id=H1gX8C4YPr</link>
        <description>We present Decentralized Distributed Proximal Policy Optimization (DD-PPO), a method for distributed reinforcement learning in resource-intensive simulated environments. DD-PPO is distributed (uses multiple machines), decentralized (lacks a centralized server), and synchronous (no computation is ever &quot;stale&quot;), making it conceptually simple and easy to implement. In our experiments on training virtual robots to navigate in Habitat-Sim, DD-PPO exhibits near-linear scaling -- achieving a speedup of 107x on 128 GPUs over a serial implementation. We leverage this scaling to train an agent for 2.5 Billion steps of experience (the equivalent of 80 years of human experience) -- over 6 months of GPU-time training in under 3 days of wall-clock time with 64 GPUs. This massive-scale training not only sets the state of art on Habitat Autonomous Navigation Challenge 2019, but essentially &quot;solves&quot; the task -- near-perfect autonomous navigation in an unseen environment without access to a map, directly from an RGB-D camera and a GPS+Compass sensor. Fortuitously, error vs computation exhibits a power-law-like distribution; thus, 90% of peak performance is obtained relatively early (at 100 million steps) and relatively cheaply (under 1 day with 8 GPUs). Finally, we show that the scene understanding and navigation policies learned can be transferred to other navigation tasks -- the analog of &quot;ImageNet pre-training + task-specific fine-tuning&quot; for embodied AI. Our model outperforms ImageNet pre-trained CNNs on these transfer tasks and can serve as a universal resource (all models and code are publicly available).</description>
    </item>
    
    <item>
        <title>PAC Confidence Sets for Deep Neural Networks via Calibrated Prediction</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Precision Gating: Improving Neural Network Efficiency with Dynamic Dual-Precision Activations</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Oblique Decision Trees from Derivatives of ReLU Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Span Recovery for Deep Neural Networks with Applications to Input Obfuscation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Improving Neural Language Generation with Spectrum Control</title>
        <link>https://openreview.net/pdf?id=ByxY8CNtvr</link>
        <description>Recent Transformer-based models such as Transformer-XL and BERT have achieved huge success on various natural language processing tasks. However, contextualized embeddings at the output layer of these powerful models tend to degenerate and occupy an anisotropic cone in the vector space, which is called the representation degeneration problem. In this paper, we propose a novel spectrum control approach to address this degeneration problem. The core idea of our method is to directly guide the spectra training of the output embedding matrix with a slow-decaying singular value prior distribution through a reparameterization framework. We show that our proposed method encourages isotropy of the learned word representations while maintains the modeling power of these contextual neural models. We further provide a theoretical analysis and insight on the benefit of modeling singular value distribution. We demonstrate that our spectrum control method outperforms the state-of-the-art Transformer-XL modeling for language model, and various Transformer-based models for machine translation, on common benchmark datasets for these tasks.</description>
    </item>
    
    <item>
        <title>Learn to Explain Efficiently via Neural Logic Inductive Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Improved memory in recurrent neural networks with sequential non-normal dynamics</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Neural Module Networks for Reasoning over Text</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Higher-Order Function Networks for Learning Composable 3D Object Representations</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Variational Hetero-Encoder Randomized GANs for Joint Image-Text Modeling</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Towards Fast Adaptation of Neural Architectures with Meta Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Graph Constrained Reinforcement Learning for Natural Language Action Spaces</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Prediction, Consistency, Curvature: Representation Learning for Locally-Linear Control</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Augmenting Non-Collaborative Dialog Systems with Explicit Semantic and Strategic Dialog History</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>BERTScore: Evaluating Text Generation with BERT</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Neural Execution of Graph Algorithms</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>On the Need for Topology-Aware Generative Models for Manifold-Based Defenses</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>FSNet: Compression of Deep Convolutional Neural Networks by Filter Summary</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Capsules with Inverted Dot-Product Attention Routing</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Composition-based Multi-Relational Graph Convolutional Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Gradient-Based Neural DAG Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>The Local Elasticity of Neural Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Composing Task-Agnostic Policies with Deep Reinforcement Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Convergence of Gradient Methods on Bilinear Zero-Sum Games</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Discovering Motor Programs by Recomposing Demonstrations</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Learning from Explanations with Neural Execution Tree</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Jelly Bean World: A Testbed for Never-Ending Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Coherent Gradients: An Approach to Understanding Generalization in Gradient Descent-based Optimization</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>MEMO: A Deep Network for Flexible Combination of Episodic Memories</title>
        <link>https://openreview.net/pdf?id=rJxlc0EtDr</link>
        <description>Recent research developing neural network architectures with external memory have often used the benchmark bAbI question and answering dataset which provides a challenging number of tasks requiring reasoning. Here we employed a classic associative inference task from the human neuroscience literature in order to more carefully probe the reasoning capacity of existing memory-augmented architectures. This task is thought to capture the essence of reasoning -- the appreciation of distant relationships among elements distributed across multiple facts or memories. Surprisingly, we found that current architectures struggle to reason over long distance associations. Similar results were obtained on a more complex task involving finding the shortest path between nodes in a path. We therefore developed a novel architecture, MEMO, endowed with the capacity to reason over longer distances. This was accomplished with the addition of two novel components. First, it introduces a separation between memories/facts stored in external memory and the items that comprise these facts in external memory. Second, it makes use of an adaptive retrieval mechanism, allowing a variable number of memory hops' before the answer is produced. MEMO is capable of solving our novel reasoning tasks, as well as all 20 tasks in bAbI.</description>
    </item>
    
    <item>
        <title>Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Bayesian Meta Sampling for Fast Uncertainty Adaptation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Non-Autoregressive Dialog State Tracking</title>
        <link>https://openreview.net/pdf?id=H1e_cC4twS</link>
        <description>Recent efforts in Dialogue State Tracking (DST) for task-oriented dialogues have progressed toward open-vocabulary or generation-based approaches where the models can generate slot value candidates from the dialogue history itself. These approaches have shown good performance gain, especially in complicated dialogue domains with dynamic slot values. However, they fall short in two aspects: (1) they do not allow models to explicitly learn signals across domains and slots to detect potential dependencies among \textit{(domain, slot)} pairs; and (2) existing models follow auto-regressive approaches which incur high time cost when the dialogue evolves over multiple domains and multiple turns. In this paper, we propose a novel framework of Non-Autoregressive Dialog State Tracking (NADST) which can factor in potential dependencies among domains and slots to optimize the models towards better prediction of dialogue states as a complete set rather than separate slots. In particular, the non-autoregressive nature of our method not only enables decoding in parallel to significantly reduce the latency of DST for real-time dialogue response generation, but also detect dependencies among slots at token level in addition to slot and domain level. Our empirical results show that our model achieves the state-of-the-art joint accuracy across all domains on the MultiWOZ 2.1 corpus, and the latency of our model is an order of magnitude lower than the previous state of the art as the dialogue history extends over time.</description>
    </item>
    
    <item>
        <title>Extreme Tensoring for Low-Memory Preconditioning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>RNNs Incrementally Evolving on an Equilibrium Manifold: A Panacea for Vanishing and Exploding Gradients?</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>The Early Phase of Neural Network Training</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Single Episode Policy Transfer in Reinforcement Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Generalization through Memorization: Nearest Neighbor Language Models</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Transformer-XH: Multi-Evidence Reasoning with eXtra Hop Attention</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Synthesizing Programmatic Policies that Inductively Generalize</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Decoding As Dynamic Programming For Recurrent Autoregressive Models</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Deep Double Descent: Where Bigger Models and More Data Hurt</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Intriguing Properties of Adversarial Training at Scale</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Shifted and Squeezed 8-bit Floating Point format for Low-Precision Training of Deep Neural Networks</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Distributed Bandit Learning: Near-Optimal Regret with Efficient Communication</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Biologically inspired sleep algorithm for increased generalization and adversarial robustness in deep neural networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>A Closer Look at the Optimization Landscapes of Generative Adversarial Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>On the Global Convergence of Training Deep Linear ResNets</title>
        <link>https://openreview.net/pdf?id=HJxEhREKDH</link>
        <description>We study the convergence of gradient descent (GD) and stochastic gradient descent (SGD) for training L-hidden-layer linear residual networks (ResNets). We prove that for training deep residual networks with certain linear transformations at input and output layers, which are fixed throughout training, both GD and SGD with zero initialization on all hidden weights can converge to the global minimum of the training loss. Moreover, when specializing to appropriate Gaussian random linear transformations, GD and SGD provably optimize wide enough deep linear ResNets. Compared with the global convergence result of GD for training standard deep linear networks \citep{du2019width}, our condition on the neural network width is sharper by a factor of O( L), where denotes the condition number of the covariance matrix of the training data. We further propose a modified identity input and output transformations, and show that a (d+k)-wide neural network is sufficient to guarantee the global convergence of GD/SGD, where d,k are the input and output dimensions respectively.</description>
    </item>
    
    <item>
        <title>Towards a Deep Network Architecture for Structured Smoothness</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Revisiting Self-Training for Neural Sequence Generation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Denoising and Regularization via Exploiting the Structural Bias of Convolutional Generators</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Variational Autoencoders for Highly Multivariate Spatial Point Processes Intensities</title>
        <link>https://openreview.net/pdf?id=B1lj20NFDS</link>
        <description>Multivariate spatial point process models can describe heterotopic data over space. However, highly multivariate intensities are computationally challenging due to the curse of dimensionality. To bridge this gap, we introduce a declustering based hidden variable model that leads to an efficient inference procedure via a variational autoencoder (VAE). We also prove that this model is a generalization of the VAE-based model for collaborative filtering. This leads to an interesting application of spatial point process models to recommender systems. Experimental results show the method&apos;s utility on both synthetic data and real-world data sets.</description>
    </item>
    
    <item>
        <title>Model-Augmented Actor-Critic: Backpropagating through Paths</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>LambdaNet: Probabilistic Type Inference using Graph Neural Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>From Inference to Generation: End-to-end Fully Self-supervised Generation of Human Face from Speech</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Learning from Unlabelled Videos Using Contrastive Predictive Neural 3D Mapping</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Decoupling Representation and Classifier for Long-Tailed Recognition</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Robust Reinforcement Learning for Continuous Control with Model Misspecification</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Cross-lingual Alignment vs Joint Training: A Comparative Study and A Simple Unified Framework</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Training Recurrent Neural Networks Online by Learning Explicit State Variables</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Uncertainty-guided Continual Learning with Bayesian Neural Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Curriculum Loss: Robust Learning and Generalization  against Label Corruption</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Picking Winning Tickets Before Training by Preserving Gradient Flow</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Generative Models for Effective ML on Private, Decentralized Datasets</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Inductive representation learning on temporal graphs</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>BatchEnsemble: an Alternative Approach to Efficient Ensemble and Lifelong Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Towards neural networks that provably know when they don't know</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Iterative energy-based projection on a normal data manifold for anomaly localization</title>
        <link>https://openreview.net/pdf?id=HJx81ySKwr</link>
        <description>Autoencoder reconstructions are widely used for the task of unsupervised anomaly localization. Indeed, an autoencoder trained on normal data is expected to only be able to reconstruct normal features of the data, allowing the segmentation of anomalous pixels in an image via a simple comparison between the image and its autoencoder reconstruction. In practice however, local defects added to a normal image can deteriorate the whole reconstruction, making this segmentation challenging. To tackle the issue, we propose in this paper a new approach for projecting anomalous data on a autoencoder-learned normal data manifold, by using gradient descent on an energy derived from the autoencoder&apos;s loss function. This energy can be augmented with regularization terms that model priors on what constitutes the user-defined optimal projection. By iteratively updating the input of the autoencoder, we bypass the loss of high-frequency information caused by the autoencoder bottleneck. This allows to produce images of higher quality than classic reconstructions. Our method achieves state-of-the-art results on various anomaly localization datasets. It also shows promising results at an inpainting task on the CelebA dataset.</description>
    </item>
    
    <item>
        <title>Towards Stable and Efficient Training of Verifiably Robust Neural Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Frequency-based Search-control in Dyna</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Learning representations for binary-classification without backpropagation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Physics-aware Difference Graph Networks for Sparsely-Observed Dynamics</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>HiLLoC: lossless image compression with hierarchical latent variable models</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>IMPACT: Importance Weighted Asynchronous Architectures with Clipped Target Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>On Bonus Based Exploration Methods In The Arcade Learning Environment</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Adaptive Correlated Monte Carlo for Contextual Categorical Sequence Generation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Smoothness and Stability in GANs</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>SNOW: Subscribing to Knowledge via Channel Pooling for Transfer & Lifelong Learning of Convolutional Neural Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Empirical Studies on the Properties of Linear Regions in Deep Neural Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Black-box Off-policy Estimation for Infinite-Horizon Reinforcement Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>PairNorm: Tackling Oversmoothing in GNNs</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Unsupervised Clustering using Pseudo-semi-supervised Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Simple and Effective Regularization Methods for Training on Noisily Labeled Data with Generalization Guarantee</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Controlling generative models with continuous factors of variations</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Understanding l4-based Dictionary Learning: Interpretation, Stability, and Robustness</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Quantum Algorithms for Deep Convolutional Neural Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Self-Supervised Learning of Appliance Usage</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Deep Graph Matching Consensus</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Beyond Linearization: On Quadratic and Higher-Order Approximation of Wide Neural Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Triple Wins: Boosting Accuracy, Robustness and Efficiency Together by Enabling Input-Adaptive Inference</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Neural Policy Gradient Methods: Global Optimality and Rates of Convergence</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Double Neural Counterfactual Regret Minimization</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>GraphAF: a Flow-based Autoregressive Model for Molecular Graph Generation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>The Gambler's Problem and Beyond</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Multilingual Alignment of Contextual Word Representations</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>The Curious Case of Neural Text Degeneration</title>
        <link>https://openreview.net/pdf?id=rygGQyrFvH</link>
        <description>Despite considerable advances in neural language modeling, it remains an open question what the best decoding strategy is for text generation from a language model (e.g. to generate a story). The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, maximization-based decoding methods such as beam search lead to degeneration output text that is bland, incoherent, or gets stuck in repetitive loops. To address this we propose Nucleus Sampling, a simple but effective method to draw considerably higher quality text out of neural language models than previous decoding strategies. Our approach avoids text degeneration by truncating the unreliable tail of the probability distribution, sampling from the dynamic nucleus of tokens containing the vast majority of the probability mass. To properly examine current maximization-based and stochastic decoding methods, we compare generations from each of these methods to the distribution of human text along several axes such as likelihood, diversity, and repetition. Our results show that (1) maximization is an inappropriate decoding objective for open-ended text generation, (2) the probability distributions of the best current language models have an unreliable tail which needs to be truncated during generation and (3) Nucleus Sampling is currently the best available decoding strategy for generating long-form text that is both high-quality as measured by human evaluation and as diverse as human-written text.</description>
    </item>
    
    <item>
        <title>Graph Convolutional Reinforcement Learning</title>
        <link>https://openreview.net/pdf?id=HkxdQkSYDB</link>
        <description>Learning to cooperate is crucially important in multi-agent environments. The key is to understand the mutual interplay between agents. However, multi-agent environments are highly dynamic, where agents keep moving and their neighbors change quickly. This makes it hard to learn abstract representations of mutual interplay between agents. To tackle these difficulties, we propose graph convolutional reinforcement learning, where graph convolution adapts to the dynamics of the underlying graph of the multi-agent environment, and relation kernels capture the interplay between agents by their relation representations. Latent features produced by convolutional layers from gradually increased receptive fields are exploited to learn cooperation, and cooperation is further improved by temporal relation regularization for consistency. Empirically, we show that our method substantially outperforms existing methods in a variety of cooperative scenarios.</description>
    </item>
    
    <item>
        <title>Meta-Learning Deep Energy-Based Memory Models</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning</title>
        <link>https://openreview.net/pdf?id=rkl3m1BFDB</link>
        <description>Saliency maps are frequently used to support explanations of the behavior of deep reinforcement learning (RL) agents. However, a review of how saliency maps are used in practice indicates that the derived explanations are often unfalsifiable and can be highly subjective. We introduce an empirical approach grounded in counterfactual reasoning to test the hypotheses generated from saliency maps and assess the degree to which they correspond to the semantics of RL environments. We use Atari games, a common benchmark for deep RL, to evaluate three types of saliency maps. Our results show the extent to which existing claims about Atari games can be evaluated and suggest that saliency maps are best viewed as an exploratory tool rather than an explanatory tool.</description>
    </item>
    
    <item>
        <title>Fast Neural Network Adaptation via Parameter Remapping and Architecture Search</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Guiding Program Synthesis by Learning to Generate Examples</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>SNODE: Spectral Discretization of Neural ODEs for System Identification</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Generalized Convolutional Forest Networks for Domain Generalization and Visual Recognition</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Once-for-All: Train One Network and Specialize it for Efficient Deployment</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Multi-Agent Interactions Modeling with Correlated Policies</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>PCMC-Net: Feature-based Pairwise Choice Markov Chains</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Implementing Inductive bias for different navigation tasks through diverse RNN attrractors</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Query2box: Reasoning over Knowledge Graphs in Vector Space Using Box Embeddings</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Rethinking the Hyperparameters for Fine-tuning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Plug and Play Language Models: A Simple Approach to Controlled Text Generation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Provable Benefit of Orthogonal Initialization in Optimizing Deep Linear Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>RGBD-GAN: Unsupervised 3D Representation Learning From Natural Image Datasets via RGBD Image Synthesis</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Towards Verified Robustness under Text Deletion Interventions</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Jacobian Adversarially Regularized Networks for Robustness</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Thinking While Moving: Deep Reinforcement Learning with Concurrent Control</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Evolutionary Population Curriculum for Scaling Multi-Agent Reinforcement Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Environmental drivers of systematicity and generalization in a situated agent</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Abstract Diagrammatic Reasoning with Multiplex Graph Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>A Baseline for Few-Shot Image Classification</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Pad Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>A FRAMEWORK  FOR ROBUSTNESS CERTIFICATION  OF SMOOTHED CLASSIFIERS USING  F-DIVERGENCES</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Contrastive Representation Distillation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Certified Defenses for Adversarial Patches</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Sample Efficient Policy Gradient Methods with Recursive Variance Reduction</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Deep Symbolic Superoptimization Without Human Knowledge</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Explain Your Move: Understanding Agent Actions Using Specific and Relevant Feature Attribution</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Universal Approximation with Certified Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Measuring and Improving the Use of Graph Information in Graph Neural Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>State-only Imitation with Transition Dynamics Mismatch</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Adversarial AutoAugment</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Meta Dropout: Learning to Perturb Latent Features for Generalization</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Rnyi Fair Inference</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Learning transport cost from subset correspondence</title>
        <link>https://openreview.net/pdf?id=SJlRUkrFPS</link>
        <description>Learning to align multiple datasets is an important problem with many applications, and it is especially useful when we need to integrate multiple experiments or correct for confounding. Optimal transport (OT) is a principled approach to align datasets, but a key challenge in applying OT is that we need to specify a cost function that accurately captures how the two datasets are related. Reliable cost functions are typically not available and practitioners often resort to using hand-crafted or Euclidean cost even if it may not be appropriate. In this work, we investigate how to learn the cost function using a small amount of side information which is often available. The side information we consider captures subset correspondence---i.e. certain subsets of points in the two data sets are known to be related. For example, we may have some images labeled as cars in both datasets; or we may have a common annotated cell type in single-cell data from two batches. We develop an end-to-end optimizer (OT-SI) that differentiates through the Sinkhorn algorithm and effectively learns the suitable cost function from side information. On systematic experiments in images, marriage-matching and single-cell RNA-seq, our method substantially outperform state-of-the-art benchmarks.</description>
    </item>
    
    <item>
        <title>BlockSwap: Fisher-guided Block Substitution for Network Compression on a Budget</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Variance Reduction With Sparse Gradients</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Abductive Commonsense Reasoning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Discrepancy Ratio: Evaluating Model Performance When Even Experts Disagree on the Truth</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Weakly Supervised Disentanglement with Guarantees</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Nesterov Accelerated Gradient and Scale Invariance for Adversarial Attacks</title>
        <link>https://openreview.net/pdf?id=SJlHwkBYDH</link>
        <description>Deep learning models are vulnerable to adversarial examples crafted by applying human-imperceptible perturbations on benign inputs. However, under the black-box setting, most existing adversaries often have a poor transferability to attack other defense models. In this work, from the perspective of regarding the adversarial example generation as an optimization process, we propose two new methods to improve the transferability of adversarial examples, namely Nesterov Iterative Fast Gradient Sign Method (NI-FGSM) and Scale-Invariant attack Method (SIM). NI-FGSM aims to adapt Nesterov accelerated gradient into the iterative attacks so as to effectively look ahead and improve the transferability of adversarial examples. While SIM is based on our discovery on the scale-invariant property of deep learning models, for which we leverage to optimize the adversarial perturbations over the scale copies of the input images so as to avoid &quot;overfitting on the white-box model being attacked and generate more transferable adversarial examples. NI-FGSM and SIM can be naturally integrated to build a robust gradient-based attack to generate more transferable adversarial examples against the defense models. Empirical results on ImageNet dataset demonstrate that our attack methods exhibit higher transferability and achieve higher attack success rates than state-of-the-art gradient-based attacks.</description>
    </item>
    
    <item>
        <title>Fantastic Generalization Measures and Where to Find Them</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Robustness Verification for Transformers</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Network Randomization: A Simple Technique for Generalization in Deep Reinforcement Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Tensor Decompositions for Temporal Knowledge Base Completion</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>On Universal Equivariant Set Networks</title>
        <link>https://openreview.net/pdf?id=HkxTwkrKDB</link>
        <description>Using deep neural networks that are either invariant or equivariant to permutations in order to learn functions on unordered sets has become prevalent. The most popular, basic models are DeepSets (Zaheer et al. 2017) and PointNet (Qi et al. 2017). While known to be universal for approximating invariant functions, DeepSets and PointNet are not known to be universal when approximating equivariant set functions. On the other hand, several recent equivariant set architectures have been proven equivariant universal (Sannai et al. 2019, Keriven and Peyre 2019), however these models either use layers that are not permutation equivariant (in the standard sense) and/or use higher order tensor variables which are less practical. There is, therefore, a gap in understanding the universality of popular equivariant set models versus theoretical ones. In this paper we close this gap by proving that: (i) PointNet is not equivariant universal; and (ii) adding a single linear transmission layer makes PointNet universal. We call this architecture PointNetST and argue it is the simplest permutation equivariant universal model known to date. Another consequence is that DeepSets is universal, and also PointNetSeg, a popular point cloud segmentation network (used e.g., in Qi et al. 2017) is universal. The key theoretical tool used to prove the above results is an explicit characterization of all permutation equivariant polynomial layers. Lastly, we provide numerical experiments validating the theoretical results and comparing different permutation equivariant models.</description>
    </item>
    
    <item>
        <title>Provable robustness against all adversarial lp-perturbations for p1</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Don't Use Large Mini-batches, Use Local SGD</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Kernel of CycleGAN as a principal homogeneous space</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Distributionally Robust Neural Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>On Solving Minimax Optimization Locally: A Follow-the-Ridge Approach</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Hyper-SAGNN: a self-attention based graph neural network for hypergraphs</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Neural Epitome Search for Architecture-Agnostic Network Compression</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>On the Equivalence between Positional Node Embeddings and Structural Graph Representations</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Probability Calibration for Knowledge Graph Embedding Models</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Why Not to Use Zero Imputation? Correcting Sparsity Bias in Training Neural Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>DropEdge: Towards Deep Graph Convolutional Networks on Node Classification</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Masked Based Unsupervised Content Transfer</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Inductive and Unsupervised Representation Learning on Graph Structured Objects</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Batch-shaping for learning conditional channel gated networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Learning Robust Representations via Multi-View Information Bottleneck</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Deep probabilistic subsampling for task-adaptive compressed sensing</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Robust anomaly detection and backdoor attack detection via differential privacy</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Learning to Guide Random Search</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Lagrangian Fluid Simulation with Continuous Convolutions</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Reinforced Genetic Algorithm Learning for Optimizing Computation Graphs</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Compressive Transformers for Long-Range Sequence Modelling</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>A Stochastic Derivative Free Optimization Method with Momentum</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Understanding and Improving Information Transfer in Multi-Task Learning</title>
        <link>https://openreview.net/pdf?id=SylzhkBtDB</link>
        <description>We investigate multi-task learning approaches that use a shared feature representation for all tasks. To better understand the transfer of task information, we study an architecture with a shared module for all tasks and a separate output module for each task. We study the theory of this setting on linear and ReLU-activated models. Our key observation is that whether or not tasks&apos; data are well-aligned can significantly affect the performance of multi-task learning. We show that misalignment between task data can cause negative transfer (or hurt performance) and provide sufficient conditions for positive transfer. Inspired by the theoretical insights, we show that aligning tasks&apos; embedding layers leads to performance gains for multi-task training and transfer learning on the GLUE benchmark and sentiment analysis tasks; for example, we obtained a 2.35% GLUE score average improvement on 5 GLUE tasks over BERT LARGE using our alignment method. We also design an SVD-based task re-weighting scheme and show that it improves the robustness of multi-task training on a multi-label image dataset.</description>
    </item>
    
    <item>
        <title>Learning To Explore Using Active Neural SLAM</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>EMPIR: Ensembles of Mixed Precision Deep Networks for Increased Robustness Against Adversarial Attacks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Quantifying Point-Prediction Uncertainty in Neural Networks via Residual Estimation with an I/O Kernel</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>B-Spline CNNs on Lie groups</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Neural Outlier Rejection for Self-Supervised Keypoint Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Reducing Transformer Depth on Demand with Structured Dropout</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Cross-Lingual Ability of Multilingual BERT: An Empirical Study</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>SPACE: Unsupervised Object-Oriented Scene Representation via Spatial Attention and Decomposition</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Low-dimensional statistical manifold embedding of directed graphs</title>
        <link>https://openreview.net/pdf?id=SkxQp1StDH</link>
        <description>We propose a novel node embedding of directed graphs to statistical manifolds, which is based on a global minimization of pairwise relative entropy and graph geodesics in a non-linear way. Each node is encoded with a probability density function over a measurable space. Furthermore, we analyze the connection of the geometrical properties of such embedding and their efficient learning procedure. Extensive experiments show that our proposed embedding is better preserving the global geodesic information of graphs, as well as outperforming existing embedding models on directed graphs in a variety of evaluation metrics, in an unsupervised setting.</description>
    </item>
    
    <item>
        <title>Efficient Probabilistic Logic Reasoning with Graph Neural Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>GraphSAINT: Graph Sampling Based Inductive Learning Method</title>
        <link>https://openreview.net/pdf?id=BJe8pkHFwS</link>
        <description>Graph Convolutional Networks (GCNs) are powerful models for learning representations of attributed graphs. To scale GCNs to large graphs, state-of-the-art methods use various layer sampling techniques to alleviate the &quot;neighbor explosion&quot; problem during minibatch training. We propose GraphSAINT, a graph sampling based inductive learning method that improves training efficiency and accuracy in a fundamentally different way. By changing perspective, GraphSAINT constructs minibatches by sampling the training graph, rather than the nodes or edges across GCN layers. Each iteration, a complete GCN is built from the properly sampled subgraph. Thus, we ensure fixed number of well-connected nodes in all layers. We further propose normalization technique to eliminate bias, and sampling algorithms for variance reduction. Importantly, we can decouple the sampling from the forward and backward propagation, and extend GraphSAINT with many architecture variants (e.g., graph attention, jumping connection). GraphSAINT demonstrates superior performance in both accuracy and training time on five large graphs, and achieves new state-of-the-art F1 scores for PPI (0.995) and Reddit (0.970).</description>
    </item>
    
    <item>
        <title>You Only Train Once: Loss-Conditional Training of Deep Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Projection-Based Constrained Policy Optimization</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Infinite-Horizon Differentiable Model Predictive Control</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Combining Q-Learning and Search with Amortized Value Estimates</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Training Generative Adversarial Networks from Incomplete Observations using Factorised Discriminators</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Decentralized Deep Learning with Arbitrary Communication Compression</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Toward Evaluating Robustness of Deep Reinforcement Learning with Continuous Control</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Gradient 1 Regularization for Quantization Robustness</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>SpikeGrad: An ANN-equivalent Computation Model for Implementing Backpropagation with Spikes</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>On the Relationship between Self-Attention and Convolutional Layers</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Learning-Augmented Data Stream Algorithms</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Structured Object-Aware Physics Prediction for Video Modeling and Planning</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Incorporating BERT into Neural Machine Translation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>MMA Training: Direct Input Space Margin Maximization through Adversarial Training</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Infinite-horizon Off-Policy Policy Evaluation with Multiple Behavior Policies</title>
        <link>https://openreview.net/pdf?id=rkgU1gHtvr</link>
        <description>We consider off-policy policy evaluation when the trajectory data are generated by multiple behavior policies. Recent work has shown the key role played by the state or state-action stationary distribution corrections in the infinite horizon context for off-policy policy evaluation. We propose estimated mixture policy (EMP), a novel class of partially policy-agnostic methods to accurately estimate those quantities. With careful analysis, we show that EMP gives rise to estimates with reduced variance for estimating the state stationary distribution correction while it also offers a useful induction bias for estimating the state-action stationary distribution correction. In extensive experiments with both continuous and discrete environments, we demonstrate that our algorithm offers significantly improved accuracy compared to the state-of-the-art methods.</description>
    </item>
    
    <item>
        <title>vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Meta-learning curiosity algorithms</title>
        <link>https://openreview.net/pdf?id=BygdyxHFDS</link>
        <description>We hypothesize that curiosity is a mechanism found by evolution that encourages meaningful exploration early in an agent&apos;s life in order to expose it to experiences that enable it to obtain high rewards over the course of its lifetime. We formulate the problem of generating curious behavior as one of meta-learning: an outer loop will search over a space of curiosity mechanisms that dynamically adapt the agent&apos;s reward signal, and an inner loop will perform standard reinforcement learning using the adapted reward signal. However, current meta-RL methods based on transferring neural network weights have only generalized between very similar tasks. To broaden the generalization, we instead propose to meta-learn algorithms: pieces of code similar to those designed by humans in ML papers. Our rich language of programs combines neural networks with other building blocks such as buffers, nearest-neighbor modules and custom loss functions. We demonstrate the effectiveness of the approach empirically, finding two novel curiosity algorithms that perform on par or better than human-designed published curiosity algorithms in domains as disparate as grid navigation with image inputs, acrobot, lunar lander, ant and hopper.</description>
    </item>
    
    <item>
        <title>Making Efficient Use of Demonstrations to Solve Hard Exploration Problems</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Lookahead: A Far-sighted Alternative of Magnitude-based Pruning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Spike-based causal inference for weight alignment</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Empirical Bayes Transductive Meta-Learning with Synthetic Gradients</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Keep Doing What Worked: Behavior Modelling Priors for Offline Reinforcement Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Understanding the Limitations of Conditional Generative Models</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Demystifying Inter-Class Disentanglement</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Mixed-curvature Variational Autoencoders</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Model-based reinforcement learning for biological sequence design</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>BayesOpt Adversarial Attack</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Meta Reinforcement Learning with Autonomous Inference of Subtask Dependencies</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Hypermodels for Exploration</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>RaPP: Novelty Detection with Reconstruction along Projection Pathway</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Dynamics-Aware Embeddings</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Functional Regularisation for  Continual Learning with Gaussian Processes</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>You CAN Teach an Old Dog New Tricks! On Training Knowledge Graph Embeddings</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>AdvectiveNet: An Eulerian-Lagrangian Fluidic Reservoir for Point Cloud Processing</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Never Give Up: Learning Directed Exploration Strategies</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Fair Resource Allocation in Federated Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Smooth markets: A basic mechanism for organizing gradient-based learners</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Training binary neural networks with real-to-binary convolutions</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Permutation Equivariant Models for Compositional Generalization in Language</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Phase Transitions for the Information Bottleneck in Representation Learning</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Variational Template Machine for Data-to-Text Generation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Memory-Based Graph Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>AtomNAS: Fine-Grained End-to-End Neural Architecture Search</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Residual Energy-Based Models for Text Generation</title>
        <link>https://openreview.net/pdf?id=B1l4SgHKDH</link>
        <description>Text generation is ubiquitous in many NLP tasks, from summarization, to dialogue and machine translation. The dominant parametric approach is based on locally normalized models which predict one word at a time. While these work remarkably well, they are plagued by exposure bias due to the greedy nature of the generation process. In this work, we investigate un-normalized energy-based models (EBMs) which operate not at the token but at the sequence level. In order to make training tractable, we first work in the residual of a pretrained locally normalized language model and second we train using noise contrastive estimation. Furthermore, since the EBM works at the sequence level, we can leverage pretrained bi-directional contextual representations, such as BERT and RoBERTa. Our experiments on two large language modeling datasets show that residual EBMs yield lower perplexity compared to locally normalized baselines. Moreover, generation via importance sampling is very efficient and of higher quality than the baseline models according to human evaluation.</description>
    </item>
    
    <item>
        <title>A closer look at the approximation capabilities of neural networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Deep Audio Priors Emerge From Harmonic Convolutional Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Expected Information Maximization: Using the I-Projection for Mixture Density Estimation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms</title>
        <link>https://openreview.net/pdf?id=ryxWIgBFPS</link>
        <description>We propose to use a meta-learning objective that maximizes the speed of transfer on a modified distribution to learn how to modularize acquired knowledge. In particular, we focus on how to factor a joint distribution into appropriate conditionals, consistent with the causal directions. We explain when this can work, using the assumption that the changes in distributions are localized (e.g. to one of the marginals, for example due to an intervention on one of the variables). We prove that under this assumption of localized changes in causal mechanisms, the correct causal graph will tend to have only a few of its parameters with non-zero gradient, i.e. that need to be adapted (those of the modified variables). We argue and observe experimentally that this leads to faster adaptation, and use this property to define a meta-learning surrogate score which, in addition to a continuous parametrization of graphs, would favour correct causal graphs. Finally, motivated by the AI agent point of view (e.g. of a robot discovering its environment autonomously), we consider how the same objective can discover the causal variables themselves, as a transformation of observed low-level variables with no causal meaning. Experiments in the two-variable case validate the proposed ideas and theoretical results.</description>
    </item>
    
    <item>
        <title>On the interaction between supervision and self-play in emergent communication</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Dynamic Model Pruning with Feedback</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Latent Normalizing Flows for Many-to-Many Cross-Domain Mappings</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Transferring Optimality Across Data Distributions via Homotopy Methods</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Regularizing activations in neural networks via distribution matching with the Wasserstein metric</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Mutual Information Gradient Estimation for  Representation Learning</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Lite Transformer with Long-Short Range Attention</title>
        <link>https://openreview.net/pdf?id=ByeMPlHKPH</link>
        <description>Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for mobile applications that are tightly constrained by the hardware resources and battery. In this paper, we present an efficient mobile NLP architecture, Lite Transformer to facilitate deploying mobile NLP applications on edge devices. The key primitive is the Long-Short Range Attention (LSRA), where one group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention). Such specialization brings consistent improvement over the vanilla transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling. Under constrained resources (500M/100M MACs), Lite Transformer outperforms transformer on WMT&apos;14 English-French by 1.2/1.7 BLEU, respectively. Lite Transformer reduces the computation of transformer base model by 2.5x with 0.3 BLEU score degradation. Combining with pruning and quantization, we further compressed the model size of Lite Transformer by 18.2x. For language modeling, Lite Transformer achieves 1.8 lower perplexity than the transformer at around 500M MACs. Notably, Lite Transformer outperforms the AutoML-based Evolved Transformer by 0.5 higher BLEU for the mobile NLP setting without the costly architecture search that requires more than 250 GPU years. Code has been made available at https://github.com/mit-han-lab/lite-transformer.</description>
    </item>
    
    <item>
        <title>A Function Space View of Bounded Norm Infinite Width ReLU Nets: The Multivariate Case</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Adversarial Lipschitz Regularization</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Compositional Language Continual Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>End to End Trainable Active Contours via Differentiable Rendering</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Provable Filter Pruning for Efficient Neural Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Effect of Activation Functions on the Training of Overparametrized Neural Nets</title>
        <link>https://openreview.net/pdf?id=rkgfdeBYvH</link>
        <description>It is well-known that overparametrized neural networks trained using gradient based methods quickly achieve small training error with appropriate hyperparameter settings. Recent papers have proved this statement theoretically for highly overparametrized networks under reasonable assumptions. These results either assume that the activation function is ReLU or they depend on the minimum eigenvalue of a certain Gram matrix. In the latter case, existing works only prove that this minimum eigenvalue is non-zero and do not provide quantitative bounds which require that this eigenvalue be large. Empirically, a number of alternative activation functions have been proposed which tend to perform better than ReLU at least in some settings but no clear understanding has emerged. This state of affairs underscores the importance of theoretically understanding the impact of activation functions on training. In the present paper, we provide theoretical results about the effect of activation function on the training of highly overparametrized 2-layer neural networks. A crucial property that governs the performance of an activation is whether or not it is smooth: For non-smooth activations such as ReLU, SELU, ELU, which are not smooth because there is a point where either the rst order or second order derivative is discontinuous, all eigenvalues of the associated Gram matrix are large under minimal assumptions on the data. For smooth activations such as tanh, swish, polynomial, which have derivatives of all orders at all points, the situation is more complex: if the subspace spanned by the data has small dimension then the minimum eigenvalue of the Gram matrix can be small leading to slow training. But if the dimension is large and the data satis es another mild condition, then the eigenvalues are large. If we allow deep networks, then the small data dimension is not a limitation provided that the depth is suf cient. We discuss a number of extensions and applications of these results.</description>
    </item>
    
    <item>
        <title>Lipschitz constant estimation of Neural Networks via sparse polynomial optimization</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>State Alignment-based Imitation Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Learning to Group: A Bottom-Up Framework for 3D Part Discovery in Unseen Categories</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Discriminative Particle Filter Reinforcement Learning for Complex Partial observations</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Unrestricted Adversarial Examples via Semantic Manipulation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Classification-Based Anomaly Detection for General Data</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Scale-Equivariant Steerable Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>On Generalization Error Bounds of Noisy Gradient Methods for Non-Convex Learning</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Consistency Regularization for Generative Adversarial Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Differentiable learning of numerical rules in knowledge graphs</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Learning to Move with Affordance Maps</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Neural tangent kernels, transportation mappings, and universal approximation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>SCALOR: Generative World Models with Scalable Object Representations</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Prediction Poisoning: Towards Defenses Against DNN Model Stealing Attacks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Domain Adaptive Multibranch Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>DiffTaichi: Differentiable Programming for Physical Simulation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Episodic Reinforcement Learning with Associative Memory</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Sub-policy Adaptation for Hierarchical Reinforcement Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Critical initialisation in continuous approximations of binary neural networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Deep Orientation Uncertainty Learning based on a Bingham Loss</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Co-Attentive Equivariant Neural Networks: Focusing Equivariance On Transformations Co-Occurring in Data</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Mixed Precision DNNs: All you need is a good parametrization</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Information Geometry of Orthogonal Initializations and Training</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Extreme Classification via Adversarial Softmax Approximation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Learning Nearly Decomposable Value Functions Via Communication Minimization</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Robust Subspace Recovery Layer for Unsupervised Anomaly Detection</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Learning to Coordinate Manipulation Skills via Skill Behavior Diversification</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>NAS-Bench-1Shot1: Benchmarking and Dissecting One-shot Neural Architecture Search</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Conservative Uncertainty Estimation By Fitting  Prior Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Understanding Generalization in Recurrent Neural Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>The Shape of Data: Intrinsic Distance for Data Distributions</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>How to 0wn the NAS in Your Spare Time</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>BREAKING CERTIFIED DEFENSES: SEMANTIC ADVERSARIAL EXAMPLES WITH SPOOFED ROBUSTNESS CERTIFICATES</title>
        <link>https://openreview.net/pdf?id=HJxdTxHYvB</link>
        <description>Defenses against adversarial attacks can be classified into certified and non-certified. Certifiable defenses make networks robust within a certain p-bounded radius, so that it is impossible for the adversary to make adversarial examples in the certificate bound. We present an attack that maintains the imperceptibility property of adversarial examples while being outside of the certified radius. Furthermore, the proposed &quot;Shadow Attack&quot; can fool certifiably robust networks by producing an imperceptible adversarial example that gets misclassified and produces a strong ``spoofed&apos;&apos; certificate.</description>
    </item>
    
    <item>
        <title>Query-efficient Meta Attack to Deep Neural Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Massively Multilingual Sparse Word Representations</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Monotonic Multihead Attention</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Gradients as Features for Deep Representation Learning</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Pay Attention to Features, Transfer Learn Faster CNNs</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Continual learning with hypernetworks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Program Guided Agent</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Sparse Coding with Gated Learned ISTA</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Graph Neural Networks Exponentially Lose Expressive Power for Node Classification</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Multi-Scale Representation Learning  for Spatial Feature Distributions using Grid Cells</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>On Robustness of Neural Ordinary Differential Equations</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Defending Against Physically Realizable Attacks on Image Classification</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Estimating Gradients for Discrete Random Variables by Sampling without Replacement</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Learning to Control PDEs with Differentiable Physics</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Intensity-Free Learning of Temporal Point Processes</title>
        <link>https://openreview.net/pdf?id=HygOjhEYDH</link>
        <description>Temporal point processes are the dominant paradigm for modeling sequences of events happening at irregular intervals. The standard way of learning in such models is by estimating the conditional intensity function. However, parameterizing the intensity function usually incurs several trade-offs. We show how to overcome the limitations of intensity-based approaches by directly modeling the conditional distribution of inter-event times. We draw on the literature on normalizing flows to design models that are flexible and efficient. We additionally propose a simple mixture model that matches the flexibility of flow-based models, but also permits sampling and computing moments in closed form. The proposed models achieve state-of-the-art performance in standard prediction tasks and are suitable for novel applications, such as learning sequence embeddings and imputing missing data.</description>
    </item>
    
    <item>
        <title>A Signal Propagation Perspective for Pruning Neural Networks at Initialization</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Skip Connections Matter: On the Transferability of Adversarial Examples Generated with ResNets</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>White Noise Analysis of Neural Networks</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Neural Machine Translation with Universal Visual Representation</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Tranquil Clouds: Neural Networks for Learning Temporally Coherent Features in Point Clouds</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Online and stochastic optimization beyond Lipschitz continuity: A Riemannian approach</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Enhancing Adversarial Defense by k-Winners-Take-All</title>
        <link>https://openreview.net/pdf?id=Skgvy64tvr</link>
        <description>We propose a simple change to existing neural network structures for better defending against gradient-based adversarial attacks. Instead of using popular activation functions (such as ReLU), we advocate the use of k-Winners-Take-All (k-WTA) activation, a C0 discontinuous function that purposely invalidates the neural network model's gradient at densely distributed input data points. The proposed k-WTA activation can be readily used in nearly all existing networks and training methods with no significant overhead. Our proposal is theoretically rationalized. We analyze why the discontinuities in k-WTA networks can largely prevent gradient-based search of adversarial examples and why they at the same time remain innocuous to the network training. This understanding is also empirically backed. We test k-WTA activation on various network structures optimized by a training method, be it adversarial training or not. In all cases, the robustness of k-WTA networks outperforms that of traditional networks under white-box attacks.</description>
    </item>
    
    <item>
        <title>Encoding word order in complex embeddings</title>
        <link>https://openreview.net/pdf?id=Hke-WTVtwr</link>
        <description>Sequential word order is important when processing text. Currently, neural networks (NNs) address this by modeling word position using position embeddings. The problem is that position embeddings capture the position of individual words, but not the ordered relationship (e.g., adjacency or precedence) between individual word positions. We present a novel and principled solution for modeling both the global absolute positions of words and their order relationships. Our solution generalizes word embeddings, previously defined as independent vectors, to continuous word functions over a variable (position). The benefit of continuous functions over variable positions is that word representations shift smoothly with increasing positions. Hence, word representations in different positions can correlate with each other in a continuous function. The general solution of these functions can be extended to complex-valued variants. We extend CNN, RNN and Transformer NNs to complex-valued versions to incorporate our complex embedding (we make all code available). Experiments on text classification, machine translation and language modeling show gains over both classical word embeddings and position-enriched word embeddings. To our knowledge, this is the first work in NLP to link imaginary numbers in complex-valued representations to concrete meanings (i.e., word order).</description>
    </item>
    
    <item>
        <title>DDSP: Differentiable Digital Signal Processing</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Cross-Domain Few-Shot Classification via Learned Feature-Wise Transformation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Ridge Regression: Structure, Cross-Validation, and Sketching</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Finite Depth and Width Corrections to the Neural Tangent Kernel</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Meta-Learning without Memorization</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Influence-Based Multi-Agent Exploration</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>HOPPITY: LEARNING GRAPH TRANSFORMATIONS TO DETECT AND FIX BUGS IN PROGRAMS</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Sliced Cramer Synaptic Consolidation for Preserving Deeply Learned Representations</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>How much Position Information Do Convolutional Neural Networks Encode?</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Hamiltonian Generative Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>CoPhy: Counterfactual Learning of Physical Dynamics</title>
        <link>https://openreview.net/pdf?id=SkeyppEFvS</link>
        <description>Understanding causes and effects in mechanical systems is an essential component of reasoning in the physical world. This work poses a new problem of counterfactual learning of object mechanics from visual input. We develop the CoPhy benchmark to assess the capacity of the state-of-the-art models for causal physical reasoning in a synthetic 3D environment and propose a model for learning the physical dynamics in a counterfactual setting. Having observed a mechanical experiment that involves, for example, a falling tower of blocks, a set of bouncing balls or colliding objects, we learn to predict how its outcome is affected by an arbitrary intervention on its initial conditions, such as displacing one of the objects in the scene. The alternative future is predicted given the altered past and a latent representation of the confounders learned by the model in an end-to-end fashion with no supervision. We compare against feedforward video prediction baselines and show how observing alternative experiences allows the network to capture latent physical properties of the environment, which results in significantly more accurate predictions at the level of super human performance.</description>
    </item>
    
    <item>
        <title>Estimating counterfactual treatment outcomes over time through adversarially balanced representations</title>
        <link>https://openreview.net/pdf?id=BJg866NFvB</link>
        <description>Identifying when to give treatments to patients and how to select among multiple treatments over time are important medical problems with a few existing solutions. In this paper, we introduce the Counterfactual Recurrent Network (CRN), a novel sequence-to-sequence model that leverages the increasingly available patient observational data to estimate treatment effects over time and answer such medical questions. To handle the bias from time-varying confounders, covariates affecting the treatment assignment policy in the observational data, CRN uses domain adversarial training to build balancing representations of the patient history. At each timestep, CRN constructs a treatment invariant representation which removes the association between patient history and treatment assignments and thus can be reliably used for making counterfactual predictions. On a simulated model of tumour growth, with varying degree of time-dependent confounding, we show how our model achieves lower error in estimating counterfactuals and in choosing the correct treatment and timing of treatment than current state-of-the-art methods.</description>
    </item>
    
    <item>
        <title>Gradientless Descent: High-Dimensional Zeroth-Order Optimization</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Conditional Learning of Fair Representations</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Inductive Matrix Completion Based on Graph Neural Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Duration-of-Stay Storage Assignment under Uncertainty</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Emergence of functional and structural properties of the head direction system by optimization of recurrent neural networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Deep neuroethology of a virtual rodent</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Learning Compositional Koopman Operators for Model-Based Control</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>CLEVRER: Collision Events for Video Representation and Reasoning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>The Logical Expressiveness of Graph Neural Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>The Break-Even Point on Optimization Trajectories of Deep Neural Networks</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Disentangling neural mechanisms for perceptual grouping</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Learning to Plan in High Dimensions via Neural Exploration-Exploitation Trees</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Symplectic Recurrent Neural Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Asymptotics of Wide Networks from Feynman Diagrams</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Learning The Difference That Makes A Difference With Counterfactually-Augmented Data</title>
        <link>https://openreview.net/pdf?id=Sklgs0NFvr</link>
        <description>Despite alarm over the reliance of machine learning systems on so-called spurious patterns, the term lacks coherent meaning in standard statistical frameworks. However, the language of causality offers clarity: spurious associations are due to confounding (e.g., a common cause), but not direct or indirect causal effects. In this paper, we focus on natural language processing, introducing methods and resources for training models less sensitive to spurious patterns. Given documents and their initial labels, we task humans with revising each document so that it (i) accords with a counterfactual target label; (ii) retains internal coherence; and (iii) avoids unnecessary changes. Interestingly, on sentiment analysis and natural language inference tasks, classifiers trained on original data fail on their counterfactually-revised counterparts and vice versa. Classifiers trained on combined datasets perform remarkably well, just shy of those specialized to either domain. While classifiers trained on either original or manipulated data alone are sensitive to spurious features (e.g., mentions of genre), models trained on the combined data are less sensitive to this signal. Both datasets are publicly available.</description>
    </item>
    
    <item>
        <title>Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?</title>
        <link>https://openreview.net/pdf?id=r1genAVKPB</link>
        <description>Modern deep learning methods provide effective means to learn good representations. However, is a good representation itself sufficient for sample efficient reinforcement learning? This question has largely been studied only with respect to (worst-case) approximation error, in the more classical approximate dynamic programming literature. With regards to the statistical viewpoint, this question is largely unexplored, and the extant body of literature mainly focuses on conditions which \emph{permit} sample efficient reinforcement learning with little understanding of what are \emph{necessary} conditions for efficient reinforcement learning. This work shows that, from the statistical viewpoint, the situation is far subtler than suggested by the more traditional approximation viewpoint, where the requirements on the representation that suffice for sample efficient RL are even more stringent. Our main results provide sharp thresholds for reinforcement learning methods, showing that there are hard limitations on what constitutes good function approximation (in terms of the dimensionality of the representation), where we focus on natural representational conditions relevant to value-based, model-based, and policy-based learning. These lower bounds highlight that having a good (value-based, model-based, or policy-based) representation in and of itself is insufficient for efficient reinforcement learning, unless the quality of this approximation passes certain hard thresholds. Furthermore, our lower bounds also imply exponential separations on the sample complexity between 1) value-based learning with perfect representation and value-based learning with a good-but-not-perfect representation, 2) value-based learning and policy-based learning, 3) policy-based learning and supervised learning and 4) reinforcement learning and imitation learning.</description>
    </item>
    
    <item>
        <title>Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Network Deconvolution</title>
        <link>https://openreview.net/pdf?id=rkeu30EtvS</link>
        <description>Convolution is a central operation in Convolutional Neural Networks (CNNs), which applies a kernel to overlapping regions shifted across the image. However, because of the strong correlations in real-world image data, convolutional kernels are in effect re-learning redundant data. In this work, we show that this redundancy has made neural network training challenging, and propose network deconvolution, a procedure which optimally removes pixel-wise and channel-wise correlations before the data is fed into each layer. Network deconvolution can be efficiently calculated at a fraction of the computational cost of a convolution layer. We also show that the deconvolution filters in the first layer of the network resemble the center-surround structure found in biological neurons in the visual regions of the brain. Filtering with such kernels results in a sparse representation, a desired property that has been missing in the training of neural networks. Learning from the sparse representation promotes faster convergence and superior results without the use of batch normalization. We apply our network deconvolution operation to 10 modern neural network models by replacing batch normalization within each. Extensive experiments show that the network deconvolution operation is able to deliver performance improvement in all cases on the CIFAR-10, CIFAR-100, MNIST, Fashion-MNIST, Cityscapes, and ImageNet datasets.</description>
    </item>
    
    <item>
        <title>Neural Symbolic Reader: Scalable Integration of Distributed and Symbolic Representations for Reading Comprehension</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Real or Not Real, that is the Question</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Dream to Control: Learning Behaviors by Latent Imagination</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>A Probabilistic Formulation of Unsupervised Text Style Transfer</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Emergent Tool Use From Multi-Agent Autocurricula</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Strategies for Pre-training Graph Neural Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Behaviour Suite for Reinforcement Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>FreeLB: Enhanced Adversarial Training for Natural Language Understanding</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Kernelized Wasserstein Natural Gradient</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>And the Bit Goes Down: Revisiting the Quantization of Neural Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>A Latent Morphology Model for Open-Vocabulary Neural Machine Translation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Understanding Why Neural Networks Generalize Well Through GSNR of Parameters</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Model Based Reinforcement Learning for Atari</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Disagreement-Regularized Imitation Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Stable Rank Normalization for Improved Generalization in Neural Networks and GANs</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Measuring the Reliability of Reinforcement Learning Algorithms</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Neural Tangents: Fast and Easy Infinite Neural Networks in Python</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Self-labelling via simultaneous clustering and representation learning</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>The intriguing role of module criticality in the generalization of deep networks</title>
        <link>https://openreview.net/pdf?id=S1e4jkSKvB</link>
        <description>We study the phenomenon that some modules of deep neural networks (DNNs) are more critical than others. Meaning that rewinding their parameter values back to initialization, while keeping other modules fixed at the trained parameters, results in a large drop in the network&apos;s performance. Our analysis reveals interesting properties of the loss landscape which leads us to propose a complexity measure, called module criticality, based on the shape of the valleys that connect the initial and final values of the module parameters. We formulate how generalization relates to the module criticality, and show that this measure is able to explain the superior generalization performance of some architectures over others, whereas, earlier measures fail to do so.</description>
    </item>
    
    <item>
        <title>Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Differentiation of Blackbox Combinatorial Solvers</title>
        <link>https://openreview.net/pdf?id=BkevoJSYPB</link>
        <description>Achieving fusion of deep learning with combinatorial algorithms promises transformative changes to artificial intelligence. One possible approach is to introduce combinatorial building blocks into neural networks. Such end-to-end architectures have the potential to tackle combinatorial problems on raw input data such as ensuring global consistency in multi-object tracking or route planning on maps in robotics. In this work, we present a method that implements an efficient backward pass through blackbox implementations of combinatorial solvers with linear objective functions. We provide both theoretical and experimental backing. In particular, we incorporate the Gurobi MIP solver, Blossom V algorithm, and Dijkstra&apos;s algorithm into architectures that extract suitable features from raw inputs for the traveling salesman problem, the min-cost perfect matching problem and the shortest path problem.</description>
    </item>
    
    <item>
        <title>Scaling Autoregressive Video Models</title>
        <link>https://openreview.net/pdf?id=rJgsskrFwH</link>
        <description>Due to the statistical complexity of video, the high degree of inherent stochasticity, and the sheer amount of data, generating natural video remains a challenging task. State-of-the-art video generation models attempt to address these issues by combining sometimes complex, often video-specific neural network architectures, latent variable models, adversarial training and a range of other methods. Despite their often high complexity, these approaches still fall short of generating high quality video continuations outside of narrow domains and often struggle with fidelity. In contrast, we show that conceptually simple, autoregressive video generation models based on a three-dimensional self-attention mechanism achieve highly competitive results across multiple metrics on popular benchmark datasets for which they produce continuations of high fidelity and realism. Furthermore, we find that our models are capable of producing diverse and surprisingly realistic continuations on a subset of videos from Kinetics, a large scale action recognition dataset comprised of YouTube videos exhibiting phenomena such as camera movement, complex object interactions and diverse human movement. To our knowledge, this is the first promising application of video-generation models to videos of this complexity.</description>
    </item>
    
    <item>
        <title>The Ingredients of Real World Robotic Reinforcement Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Meta-Learning Acquisition Functions for Transfer Learning in Bayesian Optimization</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Maximum Likelihood Constraint Inference for Inverse Reinforcement Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Spectral  Embedding of Regularized Block Models</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Towards Hierarchical Importance Attribution: Explaining Compositional Semantics for Neural Sequence Models</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>word2ket: Space-efficient Word Embeddings inspired by Quantum Entanglement</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>What Can Neural Networks Reason About?</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Training individually fair ML models with sensitive subspace robustness</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Learning from Rules Generalizing Labeled Exemplars</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Directional Message Passing for Molecular Graphs</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Explanation  by Progressive  Exaggeration</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>At Stability&apos;s Edge: How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks?</title>
        <link>https://openreview.net/pdf?id=Bkeb7lHtvH</link>
        <description>Background: Recent developments have made it possible to accelerate neural networks training significantly using large batch sizes and data parallelism. Training in an asynchronous fashion, where delay occurs, can make training even more scalable. However, asynchronous training has its pitfalls, mainly a degradation in generalization, even after convergence of the algorithm. This gap remains not well understood, as theoretical analysis so far mainly focused on the convergence rate of asynchronous methods. Contributions: We examine asynchronous training from the perspective of dynamical stability. We find that the degree of delay interacts with the learning rate, to change the set of minima accessible by an asynchronous stochastic gradient descent algorithm. We derive closed-form rules on how the learning rate could be changed, while keeping the accessible set the same. Specifically, for high delay values, we find that the learning rate should be kept inversely proportional to the delay. We then extend this analysis to include momentum. We find momentum should be either turned off, or modified to improve training stability. We provide empirical experiments to validate our theoretical findings.</description>
    </item>
    
    <item>
        <title>Disentanglement by Nonlinear ICA with General Incompressible-flow Networks (GIN)</title>
        <link>https://openreview.net/pdf?id=rygeHgSFDH</link>
        <description>A central question of representation learning asks under which conditions it is possible to reconstruct the true latent variables of an arbitrarily complex generative process. Recent breakthrough work by Khemakhem et al. (2019) on nonlinear ICA has answered this question for a broad class of conditional generative processes. We extend this important result in a direction relevant for application to real-world data. First, we generalize the theory to the case of unknown intrinsic problem dimension and prove that in some special (but not very restrictive) cases, informative latent variables will be automatically separated from noise by an estimating model. Furthermore, the recovered informative latent variables will be in one-to-one correspondence with the true latent variables of the generating process, up to a trivial component-wise transformation. Second, we introduce a modification of the RealNVP invertible neural network architecture (Dinh et al. (2016)) which is particularly suitable for this type of problem: the General Incompressible-flow Network (GIN). Experiments on artificial data and EMNIST demonstrate that theoretical predictions are indeed verified in practice. In particular, we provide a detailed set of exactly 22 informative latent variables extracted from EMNIST.</description>
    </item>
    
    <item>
        <title>Kaleidoscope: An Efficient, Learnable Representation For All Structured Linear Maps</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Improving Generalization in Meta Reinforcement Learning using Learned Objectives</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Drawing Early-Bird Tickets: Toward More Efficient Training of Deep Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Truth or backpropaganda? An empirical investigation of deep learning theory</title>
        <link>https://openreview.net/pdf?id=HyxyIgHFvr</link>
        <description>We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike. In this work, we: (1) prove the widespread existence of suboptimal local minima in the loss landscape of neural networks, and we use our theory to find examples; (2) show that small-norm parameters are not optimal for generalization; (3) demonstrate that ResNets do not conform to wide-network theories, such as the neural tangent kernel, and that the interaction between skip connections and batch normalization plays a role; (4) find that rank does not correlate with generalization or robustness in a practical setting.</description>
    </item>
    
    <item>
        <title>Neural Arithmetic Units</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>DeepSphere: a graph-based spherical CNN</title>
        <link>https://openreview.net/pdf?id=B1e3OlStPB</link>
        <description>Designing a convolution for a spherical neural network requires a delicate tradeoff between efficiency and rotation equivariance. DeepSphere, a method based on a graph representation of the discretized sphere, strikes a controllable balance between these two desiderata. This contribution is twofold. First, we study both theoretically and empirically how equivariance is affected by the underlying graph with respect to the number of pixels and neighbors. Second, we evaluate DeepSphere on relevant problems. Experiments show state-of-the-art performance and demonstrates the efficiency and flexibility of this formulation. Perhaps surprisingly, comparison with previous work suggests that anisotropic filters might be an unnecessary price to pay. Our code is available at https://github.com/deepsphere.</description>
    </item>
    
    <item>
        <title>SUMO: Unbiased Estimation of Log Marginal Probability for Latent Variable Models</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Deep Learning For Symbolic Mathematics</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Making Sense of Reinforcement Learning and Probabilistic Inference</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Unbiased Contrastive Divergence Algorithm for Training Energy-Based Latent Variable Models</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>A Mutual Information Maximization Perspective of Language Representation Learning</title>
        <link>https://openreview.net/pdf?id=Syx79eBKwr</link>
        <description>We show state-of-the-art word representation learning methods maximize an objective function that is a lower bound on the mutual information between different parts of a word sequence (i.e., a sentence). Our formulation provides an alternative perspective that unifies classical word embedding models (e.g., Skip-gram) and modern contextual embeddings (e.g., BERT, XLNet). In addition to enhancing our theoretical understanding of these methods, our derivation leads to a principled framework that can be used to construct new self-supervised tasks. We provide an example by drawing inspirations from related methods based on mutual information maximization that have been successful in computer vision, and introduce a simple self-supervised objective that maximizes the mutual information between a global sentence representation and n-grams in the sentence. Our analysis offers a holistic view of representation learning methods to transfer knowledge and translate progress across multiple domains (e.g., natural language processing, computer vision, audio processing).</description>
    </item>
    
    <item>
        <title>Energy-based models for atomic-resolution protein conformations</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Generalization of Two-layer Neural Networks: An Asymptotic Viewpoint</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Reconstructing continuous distributions of 3D protein structure from cryo-EM images</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>PROGRESSIVE LEARNING AND DISENTANGLEMENT OF HIERARCHICAL REPRESENTATIONS</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>An Exponential Learning Rate Schedule for Deep Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Geom-GCN: Geometric Graph Convolutional Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>CATER: A diagnostic dataset for Compositional Actions & TEmporal Reasoning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>BackPACK: Packing more into Backprop</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>GenDICE: Generalized Offline Estimation of Stationary Values</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>Principled Weight Initialization for Hypernetworks</title>
        <link>https://openreview.net/pdf?id=H1lma24tPB</link>
        <description>Hypernetworks are meta neural networks that generate weights for a main neural network in an end-to-end differentiable manner. Despite extensive applications ranging from multi-task learning to Bayesian deep learning, the problem of optimizing hypernetworks has not been studied to date. We observe that classical weight initialization methods like Glorot &amp; Bengio (2010) and He et al. (2015), when applied directly on a hypernet, fail to produce weights for the mainnet in the correct scale. We develop principled techniques for weight initialization in hypernets, and show that they lead to more stable mainnet weights, lower training loss, and faster convergence.</description>
    </item>
    
    <item>
        <title>On the Convergence of FedAvg on Non-IID Data</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Data-dependent Gaussian Prior Objective for Language Generation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Contrastive Learning of Structured World Models</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Neural Network Branching for Neural Network Verification</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity</title>
        <link>https://openreview.net/pdf?id=BJgnXpVYwS</link>
        <description>We provide a theoretical explanation for the effectiveness of gradient clipping in training deep neural networks. The key ingredient is a new smoothness condition derived from practical neural network training examples. We observe that gradient smoothness, a concept central to the analysis of first-order optimization algorithms that is often assumed to be a constant, demonstrates significant variability along the training trajectory of deep neural networks. Further, this smoothness positively correlates with the gradient norm, and contrary to standard assumptions in the literature, it can grow with the norm of the gradient. These empirical observations limit the applicability of existing theoretical analyses of algorithms that rely on a fixed bound on smoothness. These observations motivate us to introduce a novel relaxation of gradient smoothness that is weaker than the commonly used Lipschitz smoothness assumption. Under the new condition, we prove that two popular methods, namely, gradient clipping and normalized gradient, converge arbitrarily faster than gradient descent with fixed stepsize. We further explain why such adaptively scaled gradient methods can accelerate empirical convergence and verify our results empirically in popular neural network training settings.</description>
    </item>
    
    <item>
        <title>Posterior sampling for multi-agent reinforcement learning: solving extensive games with imperfect information</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Mogrifier LSTM</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Learning Hierarchical Discrete Linguistic Units from Visually-Grounded Speech</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Mirror-Generative Neural Machine Translation</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Your classifier is secretly an energy based model and you should treat it like one</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Dynamics-Aware Unsupervised Discovery of Skills</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Optimal Strategies Against Generative Attacks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>GraphZoom: A Multi-level Spectral Approach for Accurate and Scalable Graph Embedding</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Harnessing Structures for Value-Based Planning and Reinforcement Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Comparing Rewinding and Fine-tuning in Neural Network Pruning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Meta-Q-Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Mathematical Reasoning in Latent Space</title>
        <link>https://openreview.net/pdf?id=Ske31kBtPr</link>
        <description>We design and conduct a simple experiment to study whether neural networks can perform several steps of approximate reasoning in a fixed dimensional latent space. The set of rewrites (i.e. transformations) that can be successfully performed on a statement represents essential semantic features of the statement. We can compress this information by embedding the formula in a vector space, such that the vector associated with a statement can be used to predict whether a statement can be rewritten by other theorems. Predicting the embedding of a formula generated by some rewrite rule is naturally viewed as approximate reasoning in the latent space. In order to measure the effectiveness of this reasoning, we perform approximate deduction sequences in the latent space and use the resulting embedding to inform the semantic features of the corresponding formal statement (which is obtained by performing the corresponding rewrite sequence using real formulas). Our experiments show that graph neural networks can make non-trivial predictions about the rewrite-success of statements, even when they propagate predicted latent representations for several steps. Since our corpus of mathematical formulas includes a wide variety of mathematical disciplines, this experiment is a strong indicator for the feasibility of deduction in latent space in general.</description>
    </item>
    
    <item>
        <title>A Theory of Usable Information under Computational Constraints</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Geometric Analysis of Nonconvex Optimization Landscapes for Overcomplete Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Understanding and Robustifying Differentiable Architecture Search</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>A Closer Look at Deep Policy Gradients</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Implementation Matters in Deep RL: A Case Study on PPO and TRPO</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Fast Task Inference with Variational Intrinsic Successor Features</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Learning to Balance: Bayesian Meta-Learning for Imbalanced and Out-of-distribution Tasks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>RNA Secondary Structure Prediction By Learning Unrolled Algorithms</title>
        <link>https://openreview.net/pdf?id=S1eALyrYDH</link>
        <description>In this paper, we propose an end-to-end deep learning model, called E2Efold, for RNA secondary structure prediction which can effectively take into account the inherent constraints in the problem. The key idea of E2Efold is to directly predict the RNA base-pairing matrix, and use an unrolled algorithm for constrained programming as the template for deep architectures to enforce constraints. With comprehensive experiments on benchmark datasets, we demonstrate the superior performance of E2Efold: it predicts significantly better structures compared to previous SOTA (especially for pseudoknotted structures), while being as efficient as the fastest algorithms in terms of inference time.</description>
    </item>
    
    <item>
        <title>Watch the Unobserved: A Simple Approach to Parallelizing Monte Carlo Tree Search</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Target-Embedding Autoencoders for Supervised Representation Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Reformer: The Efficient Transformer</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Rotation-invariant clustering of neuronal responses in primary visual cortex</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Causal Discovery with Reinforcement Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Intrinsically Motivated Discovery of Diverse Patterns in Self-Organizing Systems</title>
        <link>https://openreview.net/pdf?id=rkg6sJHYDr</link>
        <description>In many complex dynamical systems, artificial or natural, one can observe self-organization of patterns emerging from local rules. Cellular automata, like the Game of Life (GOL), have been widely used as abstract models enabling the study of various aspects of self-organization and morphogenesis, such as the emergence of spatially localized patterns. However, findings of self-organized patterns in such models have so far relied on manual tuning of parameters and initial states, and on the human eye to identify interesting patterns. In this paper, we formulate the problem of automated discovery of diverse self-organized patterns in such high-dimensional complex dynamical systems, as well as a framework for experimentation and evaluation. Using a continuous GOL as a testbed, we show that recent intrinsically-motivated machine learning algorithms (POP-IMGEPs), initially developed for learning of inverse models in robotics, can be transposed and used in this novel application area. These algorithms combine intrinsically-motivated goal exploration and unsupervised learning of goal space representations. Goal space representations describe the interesting features of patterns for which diverse variations should be discovered. In particular, we compare various approaches to define and learn goal space representations from the perspective of discovering diverse spatially localized patterns. Moreover, we introduce an extension of a state-of-the-art POP-IMGEP algorithm which incrementally learns a goal representation using a deep auto-encoder, and the use of CPPN primitives for generating initialization parameters. We show that it is more efficient than several baselines and equally efficient as a system pre-trained on a hand-made database of patterns identified by human experts.</description>
    </item>
    
    <item>
        <title>Restricting the Flow: Information Bottlenecks for Attribution</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Building Deep Equivariant Capsule Networks</title>
        <link>None</link>
        <description>Message: unknown error: net::ERR_CONNECTION_RESET
  (Session info: chrome=89.0.4389.114)
</description>
    </item>
    
    <item>
        <title>A Generalized Training Approach for Multiagent Learning</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>High Fidelity Speech Synthesis with Adversarial Networks</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Meta-Learning with Warped Gradient Descent</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Convolutional Conditional Neural Processes</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Gradient Descent Maximizes the Margin of Homogeneous Neural Networks</title>
        <link>https://openreview.net/pdf?id=SJeLIgBKPS</link>
        <description>In this paper, we study the implicit regularization of the gradient descent algorithm in homogeneous neural networks, including fully-connected and convolutional neural networks with ReLU or LeakyReLU activations. In particular, we study the gradient descent or gradient flow (i.e., gradient descent with infinitesimal step size) optimizing the logistic loss or cross-entropy loss of any homogeneous model (possibly non-smooth), and show that if the training loss decreases below a certain threshold, then we can define a smoothed version of the normalized margin which increases over time. We also formulate a natural constrained optimization problem related to margin maximization, and prove that both the normalized margin and its smoothed version converge to the objective value at a KKT point of the optimization problem. Our results generalize the previous results for logistic regression with one-layer or multi-layer linear networks, and provide more quantitative convergence results with weaker assumptions than previous results for homogeneous smooth neural networks. We conduct several experiments to justify our theoretical finding on MNIST and CIFAR-10 datasets. Finally, as margin is closely related to robustness, we discuss potential benefits of training longer for improving the robustness of the model.</description>
    </item>
    
    <item>
        <title>Adversarial Training and Provable Defenses: Bridging the Gap</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Differentiable Reasoning over a Virtual Knowledge Base</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
    <item>
        <title>Federated Learning with Matched Averaging</title>
        <link>None</link>
        <description>list index out of range</description>
    </item>
    
</channel>
</rss>